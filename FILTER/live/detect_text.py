import json
import re
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoConfig, BertPreTrainedModel, BertModel

# âœ… ì„¤ì •
LABEL_NAMES = ["ìš•ì„¤", "ì„±í¬ë¡±", "í˜‘ë°•", "ì •ìƒ"]
MODEL_PATH = "./model/kobert_v12"
THRESHOLD = 0.9
DELTA_THRESHOLD = 0.2
NORMAL_CLASS_INDEX = 3

# âœ… ë‹¨ì–´ ì‚¬ì „ ë¡œë”©
with open("data/badwords.json", encoding="utf-8") as f:
    BADWORDS = set(json.load(f)["badwords"])

with open("data/force_block.json", encoding="utf-8") as f:
    FORCE_BLOCK = set(json.load(f)["force_block"])
    
# âœ… ì „ì²˜ë¦¬
def normalize(text: str) -> str:
    return re.sub(r"[^ê°€-í£a-zA-Z0-9\s]", "", text).lower().strip()

def contains_badword(norm_text: str):
    return [word for word in BADWORDS if word in norm_text]

# âœ… KoBERT ëª¨ë¸ ì •ì˜
class KoBERTClassifier(BertPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.bert = BertModel(config)
        self.dropout = nn.Dropout(0.2)
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)

    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
        pooled_output = self.dropout(outputs.pooler_output)
        logits = self.classifier(pooled_output)
        return logits

# âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©
tokenizer = AutoTokenizer.from_pretrained("monologg/kobert", trust_remote_code=True)
config = AutoConfig.from_pretrained(MODEL_PATH)
model = KoBERTClassifier.from_pretrained(MODEL_PATH, config=config)
model.eval()

# âœ… ì˜ˆì¸¡ í•¨ìˆ˜
def predict(text: str):
    inputs = tokenizer(text, return_tensors="pt", padding="max_length", truncation=True, max_length=64)
    with torch.no_grad():
        logits = model(**inputs)
        probs = F.softmax(logits, dim=1)[0].tolist()
    pred_idx = int(torch.argmax(logits, dim=1).item())
    pred_label = LABEL_NAMES[pred_idx]
    return probs, pred_label, pred_idx

# âœ… ìµœì¢… íŒë³„ í•¨ìˆ˜
def is_abuse(text: str):
    norm = normalize(text)
    detected = contains_badword(norm)

    # ê°•ì œ ì°¨ë‹¨ ë‹¨ì–´ ê°ì§€
    if any(word in FORCE_BLOCK for word in detected):
        return True, detected, "ìš•ì„¤(ê°•ì œì°¨ë‹¨)", [1.0, 0.0, 0.0, 0.0]

    probs, pred_label, pred_idx = predict(text)
    max_prob = max(probs[:NORMAL_CLASS_INDEX])
    normal_prob = probs[NORMAL_CLASS_INDEX]
    delta = max_prob - normal_prob

    is_abusive = (
        pred_idx != NORMAL_CLASS_INDEX and
        max_prob > THRESHOLD and
        delta > DELTA_THRESHOLD and
        normal_prob < 0.3
    )
    
    # ì˜¤íƒ ë°©ì§€ ë¡œì§
    if is_abusive and not detected:
        if max_prob < 0.97 or normal_prob > 0.2:
            is_abusive = False

    return is_abusive, detected, pred_label, probs

# âœ… í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    print("ğŸ“¨ í…ŒìŠ¤íŠ¸í•  ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: 'exit'):")
    while True:
        try:
            text = input(">>> ")
            if text.lower().strip() == "exit":
                break

            norm = normalize(text)
            print(f"ğŸ” ì •ê·œí™”ëœ í…ìŠ¤íŠ¸: '{norm}'")

            is_abusive, detected, label, probs = is_abuse(text)

            if label == "ìš•ì„¤(ê°•ì œì°¨ë‹¨)":
                print("ğŸš¨ íŒë³„ ê²°ê³¼: ìš•ì„¤(ê°•ì œì°¨ë‹¨) ê°ì§€ë¨")
            else:
                print(f"ğŸ“ˆ ì˜ˆì¸¡ í™•ë¥ : {probs}")
                if is_abusive:
                    print(f"ğŸš¨ íŒë³„ ê²°ê³¼: {label} ê°ì§€ë¨")
                else:
                    print("âœ… íŒë³„ ê²°ê³¼: ì •ìƒ ë¬¸ì¥ì…ë‹ˆë‹¤.")

            if detected:
                print(f"ğŸ™Š ê°ì§€ëœ ìš•ì„¤ ë‹¨ì–´: {', '.join(detected)}")

            print()
        except Exception as e:
            print(f"â— ì˜¤ë¥˜ ë°œìƒ: {e}")
