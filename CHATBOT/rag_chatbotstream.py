import os
import json
from dotenv import load_dotenv
from fastapi import APIRouter
from pydantic import BaseModel
from sse_starlette.sse import EventSourceResponse
from openai import OpenAI
from pinecone import Pinecone
import re

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# 2. ë¼ìš°í„° ì´ˆê¸°í™”
router = APIRouter()

# 3. OpenAI & Pinecone ì´ˆê¸°í™”
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))

# ì˜¤ë ˆê³¤(us-west-2) ì¸ë±ìŠ¤ ì‚¬ìš©
index_name = os.getenv("PINECONE_INDEX", "legal-guideline-usw2")
index = pc.Index(index_name)

# ì¼ìƒì ì¸ ëŒ€í™”(smalltalk)ë¡œ ë¶„ë¥˜í•  í‚¤ì›Œë“œ
SMALLTALK_KWS = [
    "ì•ˆë…•", "ì•ˆë‡½", "í•˜ì´", "hi", "hello", "í—¬ë¡œ", "í—¤ì´", "ë°©ê°€","ã…ã…‡", "ê·¸ëƒ¥",
    "ì˜ ì§€ë‚´", "ë­í•´", "ì‹¬ì‹¬í•´","ì‹¬ì‹¬","ã…ã…", "ã…‹ã…‹", "êµ¿ëª¨ë‹", "êµ¿ë°¤", "ì˜ì","ì¢‹ì€ ì•„ì¹¨", "ìˆ˜ê³ ", "ê³ ë§ˆì›Œ","ë•¡í", "ê°ì‚¬", "thanks", "thx","ã„³", "í…ŒìŠ¤íŠ¸"
]

# ì…ë ¥ ë¬¸ì¥ì´ smalltalk(ì¼ìƒ ëŒ€í™”)ì¸ì§€ íŒë³„
def is_smalltalk(text: str) -> bool:
    t = (text or "").strip().lower()
    # í‚¤ì›Œë“œ ì¤‘ í•˜ë‚˜ë¼ë„ ë“¤ì–´ê°€ë©´ ë¬´ì¡°ê±´ smalltalk
    return any(k in t for k in SMALLTALK_KWS)

# smalltalk ìœ í˜•ì— ë”°ë¼ ì ì ˆí•œ ë‹µë³€ ìƒì„±
def smalltalk_reply(text: str) -> str:
    t = (text or "").lower()
    if any(k in t for k in ["ì•ˆë…•", "ì•ˆë‡½", "í•˜ì´", "hello", "hi", "í—¬ë¡œ", "í—¤ì´","ë°©ê°€","ã…ã…‡"]):
        return "ì•ˆë…•í•˜ì„¸ìš”! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš” ğŸ˜Š ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
    if any(k in t for k in ["êµ¿ëª¨ë‹","ì¢‹ì€ ì•„ì¹¨"]):
        return "ì•ˆë…•í•˜ì„¸ìš”! ì˜ ì§€ë‚´ì…¨ë‚˜ìš”? ğŸ˜Š ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
    if any(k in t for k in ["êµ¿ë°¤","ì˜ì"]):
        return "ê³ ë§ˆì›Œìš”! í¸ì•ˆí•œ ë°¤ ë˜ì„¸ìš” ğŸŒ›"
    if any(k in t for k in ["ê³ ë§ˆì›Œ", "ê°ì‚¬", "ë•¡í","thx", "thanks","ìˆ˜ê³ ","ã„³"]):
        return "ë³„ë§ì”€ì„ìš”! ë„ì›€ì´ ë˜ì–´ ê¸°ë»ìš”. ë˜ ê¶ê¸ˆí•œ ì  ìˆìœ¼ë©´ í¸í•˜ê²Œ ë¬¼ì–´ë³´ì„¸ìš”."
    if any(k in t for k in ["ë­í•´","ì‹¬ì‹¬í•´","ì‹¬ì‹¬"]):
        return "ì—¬ê¸° ìˆì–´ìš”! ì§ˆë¬¸ì„ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘ì´ì—ìš”. ì–´ë–¤ ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”?"
    if any(k in t for k in ["ã…ã…", "ã…‹ã…‹","ê·¸ëƒ¥"]):
        return "í—¤í—¤ ğŸ˜„ ë†ë‹´ë„ ì¢‹ì•„ìš”. ì´ì œ ë³¸ë¡ ìœ¼ë¡œâ€”ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
    if "í…ŒìŠ¤íŠ¸" in t: return "ê°œë°œí•˜ëŠë¼ ê³ ìƒì´ ë§ì•„ìš”. ê·¸ë˜ë„ ëê¹Œì§€ íŒŒì´íŒ…!ğŸ’ª"
    # ê¸°ë³¸
    return "ì•ˆë…•í•˜ì„¸ìš”! í¸í•˜ê²Œ ë§ì”€í•´ ì£¼ì„¸ìš”. ë¯¼ì›/ìƒë‹´ ê´€ë ¨ë„ ì¢‹ê³ , ì¼ë°˜ì ì¸ ì§ˆë¬¸ë„ í™˜ì˜í•´ìš”."

# 4. ìš”ì²­ ëª¨ë¸ ì •ì˜  (í´ë¼ì´ì–¸íŠ¸ì—ì„œ ë“¤ì–´ì˜¤ëŠ” ë°ì´í„° í˜•ì‹)
class Query(BaseModel):
    session_id: int
    question: str

# ---------- ìœ í‹¸: í‚¤ì›Œë“œ ê¸°ë°˜ ë²•ë¥ ìŒ(1ì°¨ ìš°ì„ ) ----------

# ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ê°ì§€ í›„ (ìœ í˜•, ê´€ë ¨ë²•ë¥ ) ìŒ ì¶”ì¶œ (1ì°¨ ë§¤í•‘)
def keyword_pairs_first(text: str):
    """
    ì§ˆë¬¸(ë˜ëŠ” ìŠ¤í¬ë¦½íŠ¸)ì—ì„œ í‚¤ì›Œë“œë¥¼ ê°ì§€í•´
    sourcePagesì— ë¨¼ì € ë“¤ì–´ê°ˆ {ìœ í˜•,ê´€ë ¨ë²•ë¥ } ìŒì„ ë¦¬í„´.
    """
    hay = (text or "")
    out = []

    def add(u, l):
        out.append({"ìœ í˜•": u, "ê´€ë ¨ë²•ë¥ ": l})

    if any(k in hay for k in ["ì„±í¬ë¡±", "ìŒë€", "ìŒë‹´"]):
        add("ì„±í¬ë¡±/ìŒë€ë°œì–¸", "ì„±í­ë ¥ë²”ì£„ì˜ ì²˜ë²Œ ë“±ì— ê´€í•œ íŠ¹ë¡€ë²• ì œ13ì¡°")
    if any(k in hay for k in ["ìš•ì„¤", "í˜‘ë°•", "í­ì–¸"]):
        add("í˜‘ë°•/í­í–‰(í­ì–¸) ê°€ëŠ¥ì„±", "í˜•ë²• ì œ283ì¡°(í˜‘ë°•); í˜•ë²• ì œ260ì¡°(í­í–‰)")
        add("ì—…ë¬´ë°©í•´", "í˜•ë²• ì œ314ì¡°")
    if any(k in hay for k in ["ëª¨ìš•", "ëª…ì˜ˆí›¼ì†", "í­ì–¸"]):
        add("ëª…ì˜ˆí›¼ì†Â·ëª¨ìš•Â·í­ì–¸", "í˜•ë²• ì œ307ì¡°(ëª…ì˜ˆí›¼ì†); í˜•ë²• ì œ311ì¡°(ëª¨ìš•)")
    if "ì—…ë¬´ë°©í•´" in hay:
        add("ì—…ë¬´ë°©í•´", "í˜•ë²• ì œ314ì¡°")
    if "ê°•ìš”" in hay:
        add("ê°•ìš”", "í˜•ë²• ì œ324ì¡°")
    if any(k in hay for k in ["ì¥ë‚œì „í™”", "ê´´ë¡­í˜"]):
        add("ì¥ë‚œì „í™”/ê²½ë²”", "ê²½ë²”ì£„ì²˜ë²Œë²• ì œ3ì¡° ì œ1í•­ ì œ40í˜¸")
    if any(k in hay for k in ["ë°˜ë³µì ì¸ ë¯¼ì›"]):
        add("ë°˜ë³µ(ê³ ì§ˆ.ê°•ì„±ë¯¼ì›)", "ê²½ë²”ì£„ì²˜ë²Œë²• ì œ3ì¡° ì œ1í•­ ì œ40í˜¸")
    if "ìŠ¤í† í‚¹" in hay:
        add("ìŠ¤í† í‚¹", "ìŠ¤í† í‚¹ë²”ì£„ì˜ ì²˜ë²Œ ë“±ì— ê´€í•œ ë²•ë¥  ì œ18ì¡° ì œ1í•­")

    # ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šê²Œ ìƒìœ„ 3ê°œë§Œ
    return out[:3]

# sourcePages í•­ëª© ì •ë¦¬ (ë¹ˆ ê°’ ì œê±°, ê³µë°± ì œê±°)
def _clean_pair(e):
    if not isinstance(e, dict):
        return None
    t = (e.get("ìœ í˜•") or "").strip()
    l = (e.get("ê´€ë ¨ë²•ë¥ ") or "").strip()
    if not t or not l:
        return None
    return {"ìœ í˜•": t, "ê´€ë ¨ë²•ë¥ ": l}

 # ì—¬ëŸ¬ sourcePages ë¦¬ìŠ¤íŠ¸ë¥¼ í•©ì¹˜ê³  ì¤‘ë³µ ì œê±°
def _merge_sources(primary, *others):
    """
    primary â†’ others ìˆœìœ¼ë¡œ í•©ì¹˜ë©° (ìœ í˜•,ê´€ë ¨ë²•ë¥ ) ì¤‘ë³µ ì œê±°.
    """
    seen = set()
    merged = []

    def push_list(lst):
        for e in lst or []:
            ce = _clean_pair(e)
            if not ce:
                continue
            key = (ce["ìœ í˜•"], ce["ê´€ë ¨ë²•ë¥ "])
            if key in seen:
                continue
            seen.add(key)
            merged.append(ce)

    push_list(primary)
    for o in others:
        push_list(o)
    return merged

# ë²•ë¥  í•œ ì¤„ ìš”ì•½ ì‚¬ì „ (íŠ¹ì • ì¡°í•­ ì„¤ëª…)
_LAW_BRIEFS = {
    "ì„±í­ë ¥ë²”ì£„ì˜ ì²˜ë²Œ ë“±ì— ê´€í•œ íŠ¹ë¡€ë²• ì œ13ì¡°": "í†µì‹ ìˆ˜ë‹¨ì„ ì´ìš©í•œ ìŒë€Â·ì„±ì  ìˆ˜ì¹˜ì‹¬ ìœ ë°œ í–‰ìœ„ë¥¼ ì²˜ë²Œí•©ë‹ˆë‹¤. ì´ëŠ” 2ë…„ ì´í•˜ ì§•ì—­ ë˜ëŠ” 2ì²œë§Œì› ì´í•˜ ë²Œê¸ˆí˜•ì— í•´ë‹¹í•©ë‹ˆë‹¤. ",
    "í˜•ë²• ì œ283ì¡°": "í­í–‰Â·í˜‘ë°•ìœ¼ë¡œ ìƒëŒ€ë°©ì˜ ì˜ì‚¬ê²°ì •ì„ ì œì••í•˜ëŠ” í–‰ìœ„ë¥¼ ì²˜ë²Œí•©ë‹ˆë‹¤. ì´ëŠ”  3ë…„ ì´í•˜ ì§•ì—­ ë˜ëŠ” 500ë§Œì› ì´í•˜ ë²Œê¸ˆí˜•ì— í•´ë‹¹í•©ë‹ˆë‹¤.",
    "í˜•ë²• ì œ260ì¡°": "ìƒëŒ€ë°©ì˜ ì‹ ì²´ì— ëŒ€í•´ ìœ í˜•ë ¥ì„ í–‰ì‚¬í•˜ëŠ” í­í–‰ì„ ì²˜ë²Œí•©ë‹ˆë‹¤. ì´ëŠ” 2ë…„ ì´í•˜ ì§•ì—­ ë˜ëŠ” 500ë§Œì› ì´í•˜ ë²Œê¸ˆí˜•ì— í•´ë‹¹í•©ë‹ˆë‹¤.",
    "í˜•ë²• ì œ307ì¡°": "í—ˆìœ„ ì‚¬ì‹¤ ì ì‹œ ë˜ëŠ” ì‚¬ì‹¤ ì ì‹œë¡œ íƒ€ì¸ì˜ ëª…ì˜ˆë¥¼ í›¼ì†í•˜ëŠ” í–‰ìœ„ë¥¼ ì²˜ë²Œí•©ë‹ˆë‹¤. ì´ëŠ” 2ë…„ ì´í•˜ ì§•ì—­ ë˜ëŠ” 500ë§Œì› ì´í•˜ ë²Œê¸ˆí˜•ì— í•´ë‹¹í•©ë‹ˆë‹¤.",
    "í˜•ë²• ì œ311ì¡°": "ê³µì—°íˆ ì‚¬ëŒì„ ëª¨ìš•í•˜ëŠ” í–‰ìœ„ë¥¼ ì²˜ë²Œí•©ë‹ˆë‹¤. ì´ëŠ” 1ë…„ ì´í•˜ ì§•ì—­ ë˜ëŠ” 200ë§Œì› ì´í•˜ ë²Œê¸ˆí˜•ì— í•´ë‹¹í•©ë‹ˆë‹¤.",
    "í˜•ë²• ì œ314ì¡°": "ìœ„ë ¥ ë˜ëŠ” ê¸°íƒ€ ë°©ë²•ìœ¼ë¡œ íƒ€ì¸ì˜ ì—…ë¬´ë¥¼ ë°©í•´í•˜ëŠ” í–‰ìœ„ë¥¼ ì²˜ë²Œí•©ë‹ˆë‹¤. ì´ëŠ” 5ë…„ ì´í•˜ ì§•ì—­ ë˜ëŠ” 1ì²œ5ë°±ë§Œì› ì´í•˜ ë²Œê¸ˆí˜•ì— í•´ë‹¹í•©ë‹ˆë‹¤.",
    "í˜•ë²• ì œ324ì¡°": "í­í–‰Â·í˜‘ë°• ë“±ìœ¼ë¡œ ì˜ì‚¬ì— ë°˜í•´ ì˜ë¬´ ì—†ëŠ” ì¼ì„ í•˜ê²Œ í•˜ëŠ” ê°•ìš”ë¥¼ ì²˜ë²Œí•©ë‹ˆë‹¤. ì´ëŠ” 5ë…„ ì´í•˜ ì§•ì—­ ë˜ëŠ” 3ì²œë§Œì› ì´í•˜ ë²Œê¸ˆí˜•ì— í•´ë‹¹í•©ë‹ˆë‹¤.",
    "ê²½ë²”ì£„ì²˜ë²Œë²• ì œ3ì¡° ì œ1í•­ ì œ40í˜¸": "ì •ë‹¹í•œ ì´ìœ  ì—†ì´ ë°˜ë³µì  ì „í™” ë“±ìœ¼ë¡œ ë‚¨ì„ ê´´ë¡­íˆëŠ” í–‰ìœ„ë¥¼ ì œì¬í•©ë‹ˆë‹¤. ì´ëŠ” 10ë§Œì› ì´í•˜ ë²Œê¸ˆ, êµ¬ë¥˜, ê³¼ë£Œí˜•ì— í•´ë‹¹í•©ë‹ˆë‹¤.",
    "ìŠ¤í† í‚¹ë²”ì£„ì˜ ì²˜ë²Œ ë“±ì— ê´€í•œ ë²•ë¥  ì œ18ì¡° ì œ1í•­": "ì§€ì†ì Â·ë°˜ë³µì  ìŠ¤í† í‚¹ ë²”ì£„ë¥¼ ì²˜ë²Œí•˜ê³  ë³´í˜¸ì¡°ì¹˜ë¥¼ ê·œì •í•©ë‹ˆë‹¤. ì´ëŠ” 3ë…„ ì´í•˜ ì§•ì—­ ë˜ëŠ” 3ì²œë§Œì› ì´í•˜ ë²Œê¸ˆí˜•ì— í•´ë‹¹í•©ë‹ˆë‹¤.",
    "êµ­ë¯¼ê¶Œìµìœ„ì›íšŒ ìƒë‹´ì‚¬ ë³´í˜¸ ì§€ì¹¨": "ìƒë‹´ ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ìš•ì„¤Â·í­ì–¸Â·ì„±í¬ë¡± ë“± ì•…Â·ê°•ì„± ë¯¼ì›ìœ¼ë¡œë¶€í„° ìƒë‹´ì‚¬ë¥¼ ë³´í˜¸í•˜ê¸° ìœ„í•´ ë§ˆë ¨ëœ ì œë„ì  ì§€ì¹¨ì…ë‹ˆë‹¤. ìƒë‹´ ì¢…ë£Œ ê¸°ì¤€, ê¸°ë¡ ê´€ë¦¬, ë³´í˜¸ ì¡°ì¹˜ ì ˆì°¨ ë“±ì„ ê·œì •í•©ë‹ˆë‹¤.",
    "ë¯¼ì›ì²˜ë¦¬ë²• ì œ23ì¡°": "ë™ì¼Â·ë°˜ë³µ ë¯¼ì›ì— ëŒ€í•œ ì²˜ë¦¬ ì œí•œ/ì¢…ê²° ì ˆì°¨ë¥¼ ê·œì •í•©ë‹ˆë‹¤. ê¸°ê´€ ì§€ì¹¨ì— ë”°ë¼ ë°˜ë³µ ì œê¸°ì— ëŒ€í•´ ì¢…ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
}

# í‚¤ì›Œë“œ ê¸°ë°˜ ê¸°ë³¸ ìš”ì•½(ë§¤í•‘ ì—†ì„ ë•Œ ì¤‘ë³µ ìµœì†Œí™”)
# ì‚¬ì „ì— ì—†ëŠ” ë²•ë¥ ëª…ì„ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ê°„ë‹¨ ì„¤ëª… ìƒì„±
def _brief_fallback_by_keyword(law: str) -> str:
    if "í˜‘ë°•" in law:
        return "ìƒëŒ€ë°©ì—ê²Œ ê³µí¬ì‹¬ì„ ì•¼ê¸°í•˜ëŠ” í˜‘ë°• í–‰ìœ„ë¥¼ ì²˜ë²Œí•©ë‹ˆë‹¤."
    if "í­í–‰" in law:
        return "ìƒëŒ€ë°© ì‹ ì²´ì— ëŒ€í•œ ìœ í˜•ë ¥ í–‰ì‚¬(í­í–‰)ë¥¼ ì²˜ë²Œí•©ë‹ˆë‹¤."
    if "ëª¨ìš•" in law:
        return "ê³µì—°íˆ ì‚¬ëŒì„ ëª¨ìš•í•˜ëŠ” ì–¸í–‰ì„ ì²˜ë²Œí•©ë‹ˆë‹¤."
    if "ëª…ì˜ˆí›¼ì†" in law:
        return "í—ˆìœ„ ì‚¬ì‹¤ ë˜ëŠ” ì‚¬ì‹¤ ì ì‹œì˜ ëª…ì˜ˆí›¼ì† í–‰ìœ„ë¥¼ ì²˜ë²Œí•©ë‹ˆë‹¤."
    if "í†µì‹ " in law or "ì´ìš©ìŒë€" in law or "ì„±í­ë ¥" in law:
        return "í†µì‹ ìˆ˜ë‹¨ì„ ì´ìš©í•œ ì„±ì  ìˆ˜ì¹˜ì‹¬ ìœ ë°œ í–‰ìœ„ë¥¼ ì²˜ë²Œí•©ë‹ˆë‹¤."
    if "ì—…ë¬´ë°©í•´" in law:
        return "ìœ„ë ¥ ê¸°íƒ€ ë°©ë²•ìœ¼ë¡œ íƒ€ì¸ì˜ ì—…ë¬´ë¥¼ ë°©í•´í•˜ëŠ” í–‰ìœ„ë¥¼ ì²˜ë²Œí•©ë‹ˆë‹¤."
    if "ìŠ¤í† í‚¹" in law:
        return "ì§€ì†Â·ë°˜ë³µì  ìŠ¤í† í‚¹ í–‰ìœ„ë¥¼ ì²˜ë²Œí•˜ê³  í”¼í•´ì ë³´í˜¸ë¥¼ ê·œì •í•©ë‹ˆë‹¤."
    if "êµ­ë¯¼ê¶Œìµìœ„ì›íšŒ ìƒë‹´ì‚¬ ë³´í˜¸ ì§€ì¹¨" in law:
        return "ìƒë‹´ ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ìš•ì„¤Â·í­ì–¸Â·ì„±í¬ë¡± ë“± ì•…Â·ê°•ì„± ë¯¼ì›ìœ¼ë¡œë¶€í„° ìƒë‹´ì‚¬ë¥¼ ë³´í˜¸í•˜ê¸° ìœ„í•´ ë§ˆë ¨ëœ ì œë„ì  ì§€ì¹¨ì…ë‹ˆë‹¤. ìƒë‹´ ì¢…ë£Œ ê¸°ì¤€, ê¸°ë¡ ê´€ë¦¬, ë³´í˜¸ ì¡°ì¹˜ ì ˆì°¨ ë“±ì„ ê·œì •í•©ë‹ˆë‹¤."
    return "í•´ë‹¹ ì¡°í•­ì€ ê´€ë ¨ í–‰ìœ„ë¥¼ ê·œìœ¨Â·ì œì¬í•˜ì—¬ í”¼í•´ ë°©ì§€ë¥¼ ë„ëª¨í•©ë‹ˆë‹¤."


# ë²•ë¥  ìš”ì•½ ì„¤ëª… ë°˜í™˜ (ì‚¬ì „ ë§¤í•‘ ìš°ì„ , ì—†ìœ¼ë©´ fallback)
def _brief_for_law(law: str) -> str:
    return _LAW_BRIEFS.get(law, _brief_fallback_by_keyword(law))

 # answerì˜ ë‘ ë²ˆì§¸ ë¬¸ë‹¨ì„ ìƒì„± (ìœ í˜•/ë²•ë¥  ë‚˜ì—´ + ê° ë²•ë¥  ì„¤ëª…)
def _build_second_paragraph(sources: list[dict]) -> str:
    if not sources:
        head = "ë‹¹ì‹ ì´ ìƒë‹´í•œ ë‚´ìš©ì€ **â€˜í•´ë‹¹ ìœ í˜•â€™**ì— í•´ë‹¹í•  ìˆ˜ ìˆìœ¼ë©°, ê´€ë ¨ ë²•ë¥ ë¡œëŠ” **â€˜ê´€ë ¨ ë²•ë¥ â€™**ì´ ìˆìŠµë‹ˆë‹¤."
        tail = "ê° ë²•ë¥ ì˜ êµ¬ì²´ ì ìš©ì€ ìƒí™©ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ê¸°ê´€ ì§€ì¹¨ê³¼ ë²•ë¥  ìë¬¸ì„ í•¨ê»˜ ì°¸ê³ í•˜ì‹œê¸¸ ê¶Œì¥ë“œë¦½ë‹ˆë‹¤."
        return f"{head}\n{tail}"

    typ = (sources[0].get("ìœ í˜•") or "í•´ë‹¹ ìœ í˜•").strip()
    # ë²•ë¥ ë§Œ ëª¨ì•„ ì¤‘ë³µ ì œê±°(ìˆœì„œ ìœ ì§€)
    laws = [e.get("ê´€ë ¨ë²•ë¥ ", "").strip() for e in sources if e and e.get("ê´€ë ¨ë²•ë¥ ")]
    seen, unique_laws = set(), []
    for lw in laws:
        if lw and lw not in seen:
            seen.add(lw)
            unique_laws.append(lw)

    laws_str = "â€™, â€˜".join(unique_laws) if unique_laws else "ê´€ë ¨ ë²•ë¥ "

    # ë¨¸ë¦¬ ë¬¸ì¥: ìœ í˜•/ë²•ë¥  ëª©ë¡ë§Œ êµµê²Œ
    head = f"ë‹¹ì‹ ì´ ìƒë‹´í•œ ë‚´ìš©ì€ **â€˜{typ}â€™**ì— í•´ë‹¹í•  ìˆ˜ ìˆìœ¼ë©°, ê´€ë ¨ ë²•ë¥ ë¡œëŠ” **â€˜{laws_str}â€™**ê°€ ìˆìŠµë‹ˆë‹¤."

    # ê° í•­ëª©: **ë²•ë¥ ëª…**ë§Œ êµµê²Œ + í•œ ì¤„ ì„¤ëª…, í•­ëª© ì‚¬ì´ â€˜í•œ ì¤„â€™ ê°„ê²©
    lines = [f"- **{law}**: {_brief_for_law(law)}" for law in unique_laws]
    tail = "\n".join(lines) if lines else "ìƒì„¸ ì ìš©ì€ ì‚¬ì•ˆì˜ ë§¥ë½ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

    return f"{head}\n{tail}"


# ë‹µë³€ì„ í•­ìƒ 2ë¬¸ë‹¨ êµ¬ì¡°ë¡œ ë³´ì • (ì²« ë¬¸ë‹¨ ë³´ê°•, ë‘ ë²ˆì§¸ ë¬¸ë‹¨ ì¬ì‘ì„±)
def _ensure_two_paragraphs(answer: str, final_sources: list[dict]) -> str:
    text = (answer or "").strip()
    paras = [p.strip() for p in text.split("\n\n") if p.strip()]

    if not paras:
        paras = ["ìƒí™© ê¸°ë¡, ì¦ê±° ë³´ì¡´, ìƒê¸‰ì ë³´ê³ , ì‹¬ë¦¬ì  ì•ˆì • í™•ë³´ ë“± ì¦‰ì‹œ ì¡°ì¹˜ë¥¼ ì§„í–‰í•˜ì„¸ìš”."]

    second = _build_second_paragraph(final_sources)

    if len(paras) == 1:
        paras.append(second)
    else:
        # ë¬´ì¡°ê±´ dedup ë¡œì§ì„ ê±°ì¹œ ê²°ê³¼ë¡œ êµì²´
        paras[1] = second

    first_sentences = [s for s in paras[0].split("ã€‚") if s.strip()] if "ã€‚" in paras[0] else [s for s in paras[0].split(".") if s.strip()]
    if len(first_sentences) < 4:
        supplement = "ì‚¬ê±´ ì§í›„ì—ëŠ” í†µí™” ì„ ì¢…ë£Œ ê¸°ì¤€ê³¼ ì°¨ë‹¨ ë°©ì¹¨ì„ ìˆ™ì§€í•˜ê³ , ì¬ë°œ ë°©ì§€ë¥¼ ìœ„í•´ ì•ˆë‚´ ë©˜íŠ¸ë¥¼ í™œìš©í•˜ì„¸ìš”. ë‚´ë¶€ ê¸°ë¡ ì‹œìŠ¤í…œì— ì‹œê°„Â·ìƒí™©Â·ë°œì–¸ ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ ë‚¨ê¸°ê³ , í•„ìš” ì‹œ ë³´í˜¸ ì¡°ì¹˜ë¥¼ ì¦‰ì‹œ ìš”ì²­í•˜ì„¸ìš”."
        paras[0] = (paras[0] + " " + supplement).strip()

    return "\n\n".join(paras)


# ê´€ë ¨ë²•ë¥  ì¤‘ë³µ ì œê±°í•´ì£¼ëŠ” í•¨ìˆ˜  
# ë²•ë¥ ëª… ì •ê·œí™” (ê´„í˜¸Â·ì£¼ì„ ì œê±°)
def _normalize_law_name(law: str) -> str:
    """
    ë²•ë¥ ëª… + ì¡°ë¬¸ë²ˆí˜¸ë§Œ ë‚¨ê¸°ê³  ê´„í˜¸/ì£¼ì„ì€ ì œê±°
    ì˜ˆ: 'ë¯¼ì›ì²˜ë¦¬ë²• ì œ23ì¡° (3íšŒ ì´ìƒ ë°˜ë³µ ì‹œ ì¢…ê²°)' â†’ 'ë¯¼ì›ì²˜ë¦¬ë²• ì œ23ì¡°'
    """
    if not law:
        return ""
    return re.sub(r"\s*\(.*?\)", "", law).strip()

# ìœ í˜•, ë²•ë¥  ì¤‘ë³µ í•­ëª©ì„ ì œê±° 
# sourcePages í›„ì²˜ë¦¬ (ì¤‘ë³µ ì œê±°, ìµœëŒ€ 3ê°œ ìœ ì§€)
def _post_filter_sources(sources, limit=3):
    """
    - ë²•ë¥ ëª…ë§Œ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° (ìœ í˜•ì´ ë‹¬ë¼ë„ ê°™ì€ ë²•ë¥ ì´ë©´ 1ê°œë§Œ)
    - ì§€ì¹¨/ê°€ì´ë“œ ë“± ë¹„ë²•ë¥ ë„ í—ˆìš© (ìš”êµ¬ì‚¬í•­ ë°˜ì˜)
    - ê´„í˜¸ ì„¤ëª… ì œê±°(normalize) + ; , ë¡œ ë¬¶ì¸ í•­ëª© ë¶„í• 
    - ìµœëŒ€ limitê°œ ìœ ì§€
    """
    out = []
    seen_laws = set()

    for e in sources or []:
        typ = (e.get("ìœ í˜•") or "").strip()
        raw_law = (e.get("ê´€ë ¨ë²•ë¥ ") or "").strip()
        if not typ or not raw_law or raw_law == "ì—†ìŒ":
            continue

        # ì—¬ëŸ¬ ê°œ í•œ ì¤„ì¼ ìˆ˜ ìˆìœ¼ë‹ˆ ë¶„í• 
        for lw in [x.strip() for x in re.split(r"[;,]", raw_law) if x.strip()]:
            norm = _normalize_law_name(lw)  # ê´„í˜¸/ì£¼ì„ ì œê±°
            key = norm.lower()
            if not norm:
                continue
            # ë²•ë¥  ê¸°ì¤€ìœ¼ë¡œ dedup (ìœ í˜•ì€ ë‹¬ë¼ë„ ê°™ì€ ë²•ë¥ ì´ë©´ skip)
            if key in seen_laws:
                continue
            seen_laws.add(key)
            out.append({"ìœ í˜•": typ, "ê´€ë ¨ë²•ë¥ ": norm})
            if len(out) >= limit:
                return out

    return out


# ìœ í˜•-ê´€ë ¨ë²•ë¥  ìŒ ë‹¨ìœ„ë¡œ ë¬¶ì–´ì„œ ì¤„ë°”ê¿ˆ ì‚¬ì´ì— ë„£ì–´ì£¼ëŠ” í•¨ìˆ˜
def format_sourcepages_for_answer(sources: list[dict]) -> str:
    if not sources:
        return ""

    blocks = []
    for e in sources:
        t = e.get("ìœ í˜•", "").strip()
        l = e.get("ê´€ë ¨ë²•ë¥ ", "").strip()
        if not t or not l:
            continue
        block = f"- ìœ í˜•: {t}\n- ê´€ë ¨ë²•ë¥ : {l}"
        blocks.append(block)

    return "\n\n".join(blocks)


# 5. ìœ ì‚¬ ë¬¸ë‹¨ ê²€ìƒ‰ (ë³¸ë¬¸+ë©”íƒ€ë°ì´í„° í¬í•¨)
# Pineconeì—ì„œ queryì™€ ìœ ì‚¬ ë¬¸ë‹¨ ê²€ìƒ‰ í›„ contextì™€ sourcePages ë°˜í™˜
def retrieve_context(query: str, top_k: int = 2):
    embedding = client.embeddings.create(
        input=[query],
        model="text-embedding-3-small"  # âœ… ë” ë¹ ë¥´ê³  ì €ë ´
    ).data[0].embedding

    results = index.query(vector=embedding, top_k=top_k, include_metadata=True)

    context_blocks = []
    source_pages = []
    for match in results.get("matches", []):
        meta = match.get("metadata", {}) or {}
        typ = (meta.get("ìœ í˜•") or "").strip() or "ì—†ìŒ"
        law = (meta.get("ê´€ë ¨ ë²•ë¥ ") or "").strip() or "ì—†ìŒ"

        context_blocks.append(
            f"ğŸ“Œ **ìœ í˜•:** {typ}\n"
            f"ğŸ“– ë³¸ë¬¸: {meta.get('ë³¸ë¬¸', '')}\n"
            f"âš– **ê´€ë ¨ ë²•ë¥ **: {law}\n"
            f"ğŸ“ ìš”ì•½: {meta.get('ìš”ì•½', '')}\n"
        )
        # ìµœì¢… JSONì—ì„œëŠ” 'ê´€ë ¨ë²•ë¥ '(ë„ì–´ì“°ê¸° ì—†ìŒ)
        # 'ì—†ìŒ'ì€ ì œì™¸í•´ sourcePages ì •í•©ì„± ë³´ì¥
        if law and law != "ì—†ìŒ":
            law_norm = _normalize_law_name(law)  # ì¶”ê°€: ê´„í˜¸Â·ì£¼ì„ ì œê±°
            source_pages.append({"ìœ í˜•": typ, "ê´€ë ¨ë²•ë¥ ": law_norm})

    return "\n---\n".join(context_blocks), source_pages

# 6. GPT ìŠ¤íŠ¸ë¦¬ë° + JSON ì‘ë‹µ (í‚¤ì›Œë“œ ê¸°ë°˜ ë²•ë¥ ì„ sourcePages 1ì°¨ ë°˜ì˜)
@router.post("/stream")
async def stream_chat(query: Query):
    # 0) ì¼ìƒ ëŒ€í™”ë©´ ì¦‰ì‹œ SSEë¡œ ì‘ë‹µí•˜ê³  ì¢…ë£Œ (ëª¨ë¸/RAG í˜¸ì¶œ ì—†ì´)
    if is_smalltalk(query.question):
        async def smalltalk_events():
            payload = {"answer": smalltalk_reply(query.question), "sourcePages": []}
            yield f"data: [JSON]{json.dumps(payload, ensure_ascii=False)}\n\n"
            yield "data: [END]\n\n"
        return EventSourceResponse(smalltalk_events())
    
    # RAG
    context, source_pages_rag = retrieve_context(query.question)

    # 1ì°¨: í‚¤ì›Œë“œ ê¸°ë°˜ ë²•ë¥ ìŒ
    source_pages_keywords = keyword_pairs_first(query.question)

    # í”„ë¡¬í”„íŠ¸
    prompt = f"""
ë„ˆëŠ” ì•…ì„±ë¯¼ì› ëŒ€ì‘ ë° ê´€ë ¨ ë²•ë¥  ìƒë‹´ì„ ë„ì™€ì£¼ëŠ” AIì•¼.
ì•„ë˜ ì°¸ê³  ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ìì—°ìŠ¤ëŸ½ê³  ìì„¸í•œ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•´ì¤˜.

ë°˜ë“œì‹œ JSONìœ¼ë¡œë§Œ ì¶œë ¥í•˜ê³ , ì½”ë“œ ë¸”ë¡ì€ ì“°ì§€ ë§ˆ. ëª¨ë“  ì¶œë ¥ì€ ìì—°ìŠ¤ëŸ¬ìš´ í•œê¸€ì´ì–´ì•¼ í•´.
ì˜ì–´ í† í°(ì˜ˆ: TYPE), ìë¦¬í‘œì‹œì(ì˜ˆ: {{ìœ í˜•}})ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆ. í•œêµ­ ë²•ë ¹ ê¸°ì¤€ìœ¼ë¡œ ì„¤ëª…í•´.

- answerëŠ” **ì •í™•íˆ 2ë¬¸ë‹¨**ìœ¼ë¡œ ì‘ì„±:
  (1ë¬¸ë‹¨) ì¦‰ì‹œ ì·¨í•´ì•¼ í•  êµ¬ì²´ì  ì¡°ì¹˜(ë³´ê³ Â·ê¸°ë¡Â·ì‹¬ë¦¬ì•ˆì •Â·ì°¨ë‹¨/ì„ ì¢…ë£Œ ê¸°ì¤€ ë“±)ì™€ ì‹¤ë¬´ íŒì„ **4~6ë¬¸ì¥**ìœ¼ë¡œ ì„œìˆ .
  (2ë¬¸ë‹¨) **"ë‹¹ì‹ ì´ ìƒë‹´í•œ ë‚´ìš©ì€ â€˜{{ìœ í˜•ëª…}}â€™ì— í•´ë‹¹í•  ìˆ˜ ìˆìœ¼ë©°, ê´€ë ¨ ë²•ë¥ ë¡œëŠ” â€˜{{ë²•ë¥ ëª… ì¡°ë¬¸ë²ˆí˜¸}}â€™ê°€ ìˆìŠµë‹ˆë‹¤."**ë¡œ ì‹œì‘.
         ì´ì–´ì„œ **ê° ë²•ë¥ ë§ˆë‹¤ 1ì¤„**ë¡œ í•µì‹¬ ì ìš© ì·¨ì§€ë¥¼ ë§ë¶™ì—¬ ì„¤ëª…
         (ì˜ˆ: ì„±í­ë ¥ë²”ì£„ì˜ ì²˜ë²Œ ë“±ì— ê´€í•œ íŠ¹ë¡€ë²• ì œ13ì¡°(í†µì‹ ë§¤ì²´ì´ìš©ìŒë€): í†µì‹ ìˆ˜ë‹¨ìœ¼ë¡œ ì„±ì  ìˆ˜ì¹˜ì‹¬ì„ ìœ ë°œí•˜ëŠ” í–‰ìœ„ë¥¼ ì²˜ë²Œ).
- sourcePages: ì•„ë˜ ì°¸ê³ ìë£Œ ë° ë„¤ ì¶”ë¡ ì— ë”°ë¼ 'ìœ í˜•'ê³¼ 'ê´€ë ¨ë²•ë¥ 'ë§Œ ë°°ì—´ë¡œ ì •ë¦¬(ë²•ë¥ Â·ì¡°ë¬¸ ìœ„ì£¼).


ğŸ’¡ ì°¸ê³  ìë£Œë§Œìœ¼ë¡œ ì¶©ë¶„í•˜ì§€ ì•Šì€ ê²½ìš°ì˜ ê·œì¹™:
- ë§Œì•½ ì•„ë˜ ì°¸ê³  ìë£Œì—ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ìœ í˜•/ë²•ë¥  ì •ë³´ë¥¼ ì¶©ë¶„íˆ ì°¾ì§€ ëª»í•˜ë”ë¼ë„,
  ë„¤ê°€ ê°€ì§„ ì¼ë°˜ ì§€ì‹ì— ê¸°ë°˜í•´ ì ì ˆí•œ ì•…ì„±ë¯¼ì› ìœ í˜•ê³¼ ê´€ë ¨ ë²•ë¥ (ë˜ëŠ” ì§€ì¹¨)ì„ **ì¶”ë¡ **í•´ì„œ
  answerì™€ sourcePagesì— **í•¨ê»˜ í¬í•¨**í•´ì¤˜.
- ë‹¤ë§Œ í™•ì‹¤í•˜ì§€ ì•Šì€ ê²½ìš°ì—ëŠ” "í•´ë‹¹ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤", "ê´€ë ¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤"ì²˜ëŸ¼ **ì™„ê³¡í•œ í‘œí˜„**ì„ ì‚¬ìš©í•´.
- ë²•ë¥ Â·ì¡°ë¬¸ì„ ê¸°ì¬í•  ë• ëª…ì¹­ê³¼ ì¡°ë¬¸ ë²ˆí˜¸ë¥¼ í•¨ê»˜ ì ì–´ì¤˜. (ì˜ˆ: ì„±í­ë ¥ë²”ì£„ì˜ ì²˜ë²Œ ë“±ì— ê´€í•œ íŠ¹ë¡€ë²• ì œ13ì¡°)


ì˜ˆì‹œ:
{{
  "answer": "â€¦",
  "sourcePages": [{{"ìœ í˜•":"ë°˜ë³µ ë¯¼ì›","ê´€ë ¨ë²•ë¥ ":"êµ­ë¯¼ê¶Œìµìœ„ì›íšŒ ìƒë‹´ì‚¬ ë³´í˜¸ ì§€ì¹¨"}}]
}}

### ì°¸ê³  ìë£Œ:
{context}

### ì§ˆë¬¸:
{query.question} 
"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "system",
                "content": ("ë„ˆëŠ” ì•…ì„±ë¯¼ì› ëŒ€ì‘ ê°€ì´ë“œ ë° ê´€ë ¨ ë²•ë¥  ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒë‹´í•˜ëŠ” ì „ë¬¸ê°€ AIë‹¤."
                            "ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ê³ , ì½”ë“œ ë¸”ë¡ì´ë‚˜ ë¶€ê°€ ì„¤ëª…ì€ ì ˆëŒ€ í•˜ì§€ ë§ˆ."
                            "RAG(ì°¸ê³ ìë£Œ)ì—ì„œ ì¶©ë¶„í•œ ì •ë³´ê°€ ì—†ì„ ê²½ìš°ì—ë„ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ í•©ë¦¬ì  ì¶”ë¡ ì„ í•˜ë˜, "
                            "ë¶ˆí™•ì‹¤í•œ ë¶€ë¶„ì€ ë‹¨ì •í•˜ì§€ ë§ê³  'ê´€ë ¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤' ë“± ì™„ê³¡ í‘œí˜„ì„ ì‚¬ìš©í•´."
                )
            },
            {"role": "user", "content": prompt}
        ],
        stream=True
    )

    async def event_generator():
        full_response = ""
        for chunk in response:
            delta = chunk.choices[0].delta.content
            if delta:
                full_response += delta
                yield f"data: {delta}\n\n"

        # ----- ëª¨ë¸ ì¶œë ¥ JSON ë³´ì • ë° sourcePages ìš°ì„  ë³‘í•© -----
        model_answer = full_response
        model_sources = []
        try:
            parsed = json.loads(full_response)
            if isinstance(parsed, dict):
                if "answer" in parsed and isinstance(parsed["answer"], str):
                    model_answer = parsed["answer"]
                sp = parsed.get("sourcePages")
                if isinstance(sp, list):
                    model_sources = [_clean_pair(e) for e in sp if _clean_pair(e)]
                    # ì¶”ê°€: ê´€ë ¨ë²•ë¥  ì •ê·œí™”
                    model_sources = [
                        {"ìœ í˜•": ms["ìœ í˜•"], "ê´€ë ¨ë²•ë¥ ": _normalize_law_name(ms["ê´€ë ¨ë²•ë¥ "])}
                        for ms in model_sources
                    ]

        except Exception:
            pass

        # ë³‘í•© ê·œì¹™: í‚¤ì›Œë“œ(1ì°¨) â†’ ëª¨ë¸ sourcePages â†’ RAG sourcePages
        final_sources = _merge_sources(source_pages_keywords, model_sources, source_pages_rag)
        
        # í›„ì²˜ë¦¬: ë¹„ë²•ë¥ /ì—†ìŒ ì œê±° + ìµœëŒ€ 3ê°œ ì œí•œ
        final_sources = _post_filter_sources(final_sources, limit=3)
        
        # answer 2ë¬¸ë‹¨/ì‹œì‘ë¬¸ì¥/ìš”ì•½ ê°•ì œ ë³´ì •
        final_answer = _ensure_two_paragraphs(model_answer, final_sources)

        payload = {"answer": final_answer, "sourcePages": final_sources, "sourcePagesText": format_sourcepages_for_answer(final_sources)}
        yield f"data: [JSON]{json.dumps(payload, ensure_ascii=False)}\n\n"
        yield "data: [END]\n\n"

    return EventSourceResponse(event_generator())
