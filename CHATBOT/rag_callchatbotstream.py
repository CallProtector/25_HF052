import os, json, asyncio, random, re
from collections import defaultdict, deque
from typing import Optional, Dict, List, Tuple
from dotenv import load_dotenv
from fastapi import APIRouter
from pydantic import BaseModel
from sse_starlette.sse import EventSourceResponse
from openai import OpenAI
from pinecone import Pinecone

# ê³µí†µ ì •ì±…
from reply_policy import (
    is_smalltalk, smalltalk_reply,
    keyword_pairs_first,
    parse_model_json,
    merge_sources,
    ensure_two_paragraphs,
    format_sourcepages_text,
)

load_dotenv()
router = APIRouter()

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
# ì˜¤ë ˆê³¤(us-west-2) ì¸ë±ìŠ¤ ì‚¬ìš©
index_name = os.getenv("PINECONE_INDEX", "legal-guideline-usw2")
index = pc.Index(index_name)

# ---- in-memory session ----
MAX_TURNS = 8
session_memory: Dict[str, deque] = defaultdict(lambda: deque(maxlen=MAX_TURNS))
session_locks: Dict[str, asyncio.Lock] = defaultdict(lambda: asyncio.Lock())

# ---- request model ----
class StreamQuery(BaseModel):
    session_id: int
    question: str
    # STT ìŠ¤í¬ë¦½íŠ¸: [{ "speaker": "INBOUND|OUTBOUND", "text": "..." }, ...]
    context_scripts: Optional[List[Dict[str, str]]] = None

# ---- helpers ----
def ns_key(session_id: int) -> str:
    # CALL/CHAT í†µí•© ë„¤ì„ìŠ¤í˜ì´ìŠ¤
    return "call:" + str(session_id)

# ----- canned ì‘ë‹µ (ì˜µì…˜) -----
async def slow_emit_json(payload: dict, min_wait: float = 1.8, max_wait: float = 3.0):
    await asyncio.sleep(random.uniform(min_wait, max_wait))
    yield f"data: [JSON]{json.dumps(payload, ensure_ascii=False)}\n\n"
    yield "data: [END]\n\n"

# êµì²´ (í‚¤ì›Œë“œ 2ê°œë§Œ ìˆì–´ë„ ë§¤ì¹­: "í­ì–¸"+"ìì„¸íˆ" / "ì„±í¬ë¡±"+"ìì„¸íˆ")
PATTERNS = {
    "ABUSE_DETAIL": re.compile(r"(í­ì–¸|ìš•ì„¤|í˜‘ë°•).*(ìì„¸íˆ)|ìì„¸íˆ.*(í­ì–¸|ìš•ì„¤|í˜‘ë°•)", re.I),
    "HARASS_DETAIL": re.compile(r"(ì„±í¬ë¡±|ìŒë€).*(ìì„¸íˆ)|ìì„¸íˆ.*(ì„±í¬ë¡±|ìŒë€)", re.I),
}

ABUSE_INSULT_RE = [
    r"\bì‹œë°œ\b", r"\bì”¨ë°œ\b", r"ê°œìƒˆë¼", r"ë³‘ì‹ ", r"ë“±ì‹ ", r"ë¯¸ì¹œ", r"\bêº¼ì ¸\b", r"ê°œ?ê°™ì€", r"ë¯¸ì¹œ", r"ê°œê°™ì´", r"ì´ë”´", r"í•™ë ¥", r"ê³ ì¡¸", r"ë‹ˆë„¤",
]
ABUSE_THREAT_RE = [
    r"ì£½(ì—¬|ì¸ë‹¤|ì—¬ë²„ë¦°ë‹¤)", r"ì£½ê³ \s*ì‹¶ëƒ", r"ê°€ë§Œ(ë‘ì§€|ì•ˆ[ë‘˜ë‘”ë‹¤])", r"íŒ¨ë²„ë¦°ë‹¤", r"ì°¾ì•„ê°€(ì„œ)? (ê°€ë§Œë‘ì§€|í˜¼?ë‚´|ì£½ì´)", r"ì¹¼ë“¤ê³ ",
]
SEXUAL_RE = [
    r"ì•¼í•˜(ì‹œ|ê²Œ)?", r"ëª©ì†Œë¦¬.*ì•¼í•˜",r"ëª©ì†Œë¦¬", r"ì„¹ì‹œ", r"ë°¤ì—.*í”¼ëŠ”", r"(ìŒë€|ìŒíƒ•|ìŒí‰)", r"ì„¹ìŠ¤", r"ê°€ìŠ´", r"ì—‰ë©ì´",
]

def extract_matches_from_scripts(
    scripts: Optional[List[Dict[str, str]]],
    patterns: List[str],
    speaker: str = "INBOUND",
    max_examples: int = 5
) -> List[str]:
    """ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì •ê·œì‹ íŒ¨í„´ì— ë§¤ì¹­ë˜ëŠ” ë¬¸ì¥ì„ ìµœëŒ€ max_examplesê°œ ì¶”ì¶œ"""
    out: List[str] = []
    for s in (scripts or []):
        if speaker and s.get("speaker") != speaker:
            continue
        text = (s.get("text") or "").strip()
        for pat in patterns:
            if re.search(pat, text, flags=re.IGNORECASE):
                out.append(text)
                break
        if len(out) >= max_examples:
            break
    return out

def match_canned_tag(text: str) -> Optional[str]:
    t = (text or "")
    for tag, pat in PATTERNS.items():
        if pat.search(t):
            return tag
    return None

def build_canned_payload(tag: str, scripts: Optional[List[Dict[str, str]]] = None) -> Optional[dict]:
    if tag == "ABUSE_DETAIL":
        matched = extract_matches_from_scripts(scripts, ABUSE_INSULT_RE + ABUSE_THREAT_RE)
        has_match = bool(matched)
        bullets_inline = ", ".join(f"\"**{m}**\"" for m in matched) if has_match else None
        bullets_list   = "\n".join(f'- "**{m}**"' for m in matched) if has_match else None

        src = [
            {"ìœ í˜•": "í˜‘ë°•/í­ì–¸", "ê´€ë ¨ë²•ë¥ ": "í˜•ë²• ì œ283ì¡°"},
            {"ìœ í˜•": "ì—…ë¬´ë°©í•´", "ê´€ë ¨ë²•ë¥ ": "í˜•ë²• ì œ314ì¡°"},
            {"ìœ í˜•": "ëª…ì˜ˆí›¼ì†Â·ëª¨ìš•Â·í­ì–¸", "ê´€ë ¨ë²•ë¥ ": "í˜•ë²• ì œ311ì¡°"},
            {"ìœ í˜•": "ëª…ì˜ˆí›¼ì†Â·í­ì–¸", "ê´€ë ¨ë²•ë¥ ": "í˜•ë²• ì œ307ì¡°"}, 
        ]

        # 1) ë§¤ì¹­ëœ ë¬¸ì¥ ì†Œê°œ ë‹¨ë½: ìˆì„ ë•Œë§Œ
        first_part = f"í­ì–¸(ìš•ì„¤/í˜‘ë°•)ì— í•´ë‹¹í•˜ëŠ” ë°œì–¸ì€ {bullets_inline}ê°€ ìˆì–´ìš”.\n\n" if has_match else ""

        # 2) ë²•ë¥  ì†Œê°œ ë¬¸ì¥: ë§¤ì¹­ ìˆìœ¼ë©´ â€œí•´ë‹¹ ë°œì–¸ì€ â€¦ì— í•´ë‹¹í•  ìˆ˜ ìˆìœ¼ë©°â€, ì—†ìœ¼ë©´ ì¤‘ë¦½ í‘œí˜„
        intro_line = (
            "í•´ë‹¹ ë°œì–¸ì€ **â€˜í˜‘ë°•/í­ì–¸â€™**ì— í•´ë‹¹í•  ìˆ˜ ìˆìœ¼ë©°, ê´€ë ¨ ë²•ë¥ ë¡œëŠ” "
            if has_match else
            "ìš”ì²­í•˜ì‹  ìœ í˜•(í˜‘ë°•/í­ì–¸)ì— ëŒ€í•œ **ê´€ë ¨ ë²•ë¥  ì•ˆë‚´**ì…ë‹ˆë‹¤: "
        )

        laws_block = (
            "**â€˜í˜•ë²• ì œ283ì¡°â€™**, **â€˜í˜•ë²• ì œ314ì¡°(ì—…ë¬´ë°©í•´)â€™**, **â€˜í˜•ë²• ì œ311ì¡°(ê³µì—°ì„± ì¶©ì¡± ì‹œ)â€™** ë“±ì´ ìˆìŠµë‹ˆë‹¤.\n"
            "- **í˜•ë²• ì œ283ì¡°**: ìƒëŒ€ì—ê²Œ ê³µí¬ì‹¬ì„ ìœ ë°œí•˜ëŠ” í˜‘ë°• í–‰ìœ„ë¥¼ ì²˜ë²Œí•©ë‹ˆë‹¤. (3ë…„ ì´í•˜ ì§•ì—­ ë˜ëŠ” 500ë§Œì› ì´í•˜ ë²Œê¸ˆ)\n"
            "- **í˜•ë²• ì œ314ì¡°**: ìœ„ë ¥Â·ê³ ì„±Â·ìš•ì„¤ ë“±ìœ¼ë¡œ ì •ìƒì ì¸ ì—…ë¬´ë¥¼ ë°©í•´í•œ ê²½ìš° ì²˜ë²Œí•©ë‹ˆë‹¤. (5ë…„ ì´í•˜ ì§•ì—­ ë˜ëŠ” 1ì²œ5ë°±ë§Œì› ì´í•˜ ë²Œê¸ˆ)\n"
            "- **í˜•ë²• ì œ311ì¡°**: ê³µì—°ì„±ì´ ìˆëŠ” ëª¨ìš•í–‰ìœ„ë¥¼ ì²˜ë²Œí•©ë‹ˆë‹¤. (1ë…„ ì´í•˜ ì§•ì—­ ë˜ëŠ” 200ë§Œì› ì´í•˜ ë²Œê¸ˆ)\n"
        )

        answer = (
            f"{first_part}"
            f"{intro_line}{laws_block}\n"
            "ì¦‰ì‹œ ì·¨í•´ì•¼ í•  ì¡°ì¹˜ëŠ” í­ì–¸ì„ ëª…í™•íˆ ì¸ì§€í•˜ê³  ì´ë¥¼ ê¸°ë¡í•˜ì—¬ ìƒê¸‰ìì—ê²Œ ë³´ê³ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. "
            "ìƒë‹´ì›ì€ ë¯¼ì›ì¸ì˜ ë°œì–¸ì— ëŒ€í•´ ê°ì •ì ìœ¼ë¡œ ëŒ€ì‘í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ê³ , í•„ìš” ì‹œ ë™ë£Œì™€ì˜ ìƒë‹´ì„ í†µí•´ ì‹¬ë¦¬ì  ì•ˆì •ì„ ì·¨í•´ì•¼ í•©ë‹ˆë‹¤. "
            "ë§Œì•½ ë¯¼ì›ì¸ì˜ í­ì–¸ì´ ì§€ì†ë  ê²½ìš°, ì‘ëŒ€ì¤‘ ì°¨ë‹¨Â·ì„ ì¢…ë£Œ ê¸°ì¤€ì„ ì ìš©í•˜ê³  ìƒë‹´ì› ë³´í˜¸ ê´€ì ì—ì„œ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ì¤„ì´ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. "
            "ì´ëŸ¬í•œ ì¡°ì¹˜ë¥¼ ì·¨í•˜ëŠ” ê²ƒì€ ìƒë‹´ì›ê³¼ ë¯¼ì›ì¸ ê°„ì˜ ê±´ê°•í•œ ì˜ì‚¬ì†Œí†µì„ ìœ ì§€í•˜ëŠ” ë°ë„ ë„ì›€ì´ ë©ë‹ˆë‹¤.\n\n"
            "ìƒë‹´ì›ë‹˜ì˜ ê±´ê°•í•œ ê·¼ë¬´ í™˜ê²½ì„ ì‘ì›í•©ë‹ˆë‹¤ :)\n"
        )

        return {"answer": answer, "sourcePages": src, "sourcePagesText": format_sourcepages_text(src)}

    if tag == "HARASS_DETAIL":
        matched = extract_matches_from_scripts(scripts, SEXUAL_RE)
        has_match = bool(matched)
        bullets_inline = ", ".join(f"\"**{m}**\"" for m in matched) if has_match else None
        bullets_list   = "\n".join(f'- "**{m}**"' for m in matched) if has_match else None

        src = [{"ìœ í˜•": "ì„±í¬ë¡±/ìŒë€ë°œì–¸", "ê´€ë ¨ë²•ë¥ ": "ì„±í­ë ¥ë²”ì£„ì˜ ì²˜ë²Œ ë“±ì— ê´€í•œ íŠ¹ë¡€ë²• ì œ13ì¡°"}]

        first_part = f"ì„±í¬ë¡±(ìŒë€ë°œì–¸)ì— í•´ë‹¹í•˜ëŠ” ë°œì–¸ì€ {bullets_inline}ê°€ ìˆì–´ìš”.\n\n" if has_match else ""

        intro_line = (
            "í•´ë‹¹ ë°œì–¸ì€ **â€˜ì„±í¬ë¡±/ìŒë€ë°œì–¸â€™**ì— í•´ë‹¹í•  ìˆ˜ ìˆìœ¼ë©°, ê´€ë ¨ ë²•ë¥ ë¡œëŠ” "
            if has_match else
            "ìš”ì²­í•˜ì‹  ìœ í˜•(ì„±í¬ë¡±/ìŒë€ë°œì–¸)ì— ëŒ€í•œ **ê´€ë ¨ ë²•ë¥  ì•ˆë‚´**ì…ë‹ˆë‹¤: "
        )

        laws_block = (
            "**â€˜ì„±í­ë ¥ë²”ì£„ì˜ ì²˜ë²Œ ë“±ì— ê´€í•œ íŠ¹ë¡€ë²• ì œ13ì¡°â€™**ê°€ ìˆìŠµë‹ˆë‹¤.\n"
            "- **ì„±í­ë ¥ë²”ì£„ì˜ ì²˜ë²Œ ë“±ì— ê´€í•œ íŠ¹ë¡€ë²• ì œ13ì¡°**: í†µì‹ ìˆ˜ë‹¨ì„ ì´ìš©í•œ ì„±ì  ìˆ˜ì¹˜ì‹¬ ìœ ë°œ í–‰ìœ„ë¥¼ ì²˜ë²Œí•©ë‹ˆë‹¤. (2ë…„ ì´í•˜ ì§•ì—­ ë˜ëŠ” 2ì²œë§Œì› ì´í•˜ ë²Œê¸ˆ)\n"
        )

        answer = (
            f"{first_part}"
            f"{intro_line}{laws_block}\n"
            "ì¦‰ì‹œ ì·¨í•´ì•¼ í•  ì¡°ì¹˜ëŠ” ì„±í¬ë¡± ë°œì–¸ì— ëŒ€í•´ ì¦‰ê°ì ì¸ ì¤‘ì§€ ìš”ì²­ì„ í•˜ê³ , "
            "ì´ë¥¼ ë¬¸ì„œë¡œ ê¸°ë¡í•˜ì—¬ ìƒê¸‰ìì—ê²Œ ë³´ê³ í•´ì•¼ í•©ë‹ˆë‹¤. ì´í›„, í”¼í•´ìì˜ ì‹¬ë¦¬ì  ì•ˆì •ì„ ìœ„í•´ ì „ë¬¸ ìƒë‹´ì„ ì œê³µí•´ì•¼ í•  ìˆ˜ ìˆìœ¼ë©°, "
            "ì¬ë°œí•  ê²½ìš° ARS ê²½ê³  í›„ í†µí™”ë¥¼ ì¢…ë£Œí•  ìˆ˜ ìˆëŠ” ê¸°ì¤€ì„ ë§ˆë ¨í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. "
            "ì´ëŸ¬í•œ ì ˆì°¨ë¥¼ í†µí•´ ë¯¼ì›ì¸ì˜ í–‰ë™ì— ëŒ€í•œ ê²½ê°ì‹¬ì„ ë†’ì´ê³ , í”¼í•´ì ë³´í˜¸ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ í•´ì•¼ í•©ë‹ˆë‹¤.\n\n"
            "ìƒë‹´ì›ë‹˜ì˜ ê±´ê°•í•œ ê·¼ë¬´ í™˜ê²½ì„ ì‘ì›í•©ë‹ˆë‹¤ :)\n"
        )

        return {"answer": answer, "sourcePages": src, "sourcePagesText": format_sourcepages_text(src)}

    return None


# --- helpers (ì¶”ê°€) ---
TYPE_KEYWORDS = {
    "ì„±í¬ë¡±/ìŒë€ë°œì–¸": ["ì„±í¬ë¡±","ìŒë€","ì„±ì ","ëª©ì†Œë¦¬", "ì•¼í•˜", "ì•¼í•œ","ì„¹ìŠ¤","ìŒë‹´","ìŒíƒ•"],
    "í˜‘ë°•/í­í–‰(í­ì–¸) ê°€ëŠ¥ì„±": ["ì£½ì—¬ë²„ë¦°ë‹¤","í­ì–¸","ìš•ì„¤","í˜‘ë°•","ì£½ì—¬","íŒ¨ë²„ë¦°ë‹¤","ì£½ê³  ì‹¶ëƒ","ì‹œë°œ", "ì”¨ë°œ","ã……ã…‚","ê°œìƒˆ"],
    "ëª…ì˜ˆí›¼ì†Â·ëª¨ìš•Â·í­ì–¸": ["ëª¨ìš•","ëª…ì˜ˆí›¼ì†","ë¹„ë°©","ë°”ë³´","ë©ì²­","ë³‘ì‹ ","ë“±ì‹ "],
    "ì—…ë¬´ë°©í•´": ["ì—…ë¬´ë°©í•´","ì—…ë¬´ ë°©í•´"],
    "ê°•ìš”": ["ê°•ìš”"],
    "ìŠ¤í† í‚¹": ["ìŠ¤í† í‚¹","ì§€ì† ì—°ë½","ë°˜ë³µ ì—°ë½"],
}

def detect_allowed_types(text: str) -> set[str]:
    hay = (text or "").lower()
    out = set()
    for typ, kws in TYPE_KEYWORDS.items():
        if any(k.lower() in hay for k in kws):
            out.add(typ)
    return out

def filter_sources_by_types(sources: List[dict], allowed: set[str]) -> List[dict]:
    if not allowed:
        return sources
    return [s for s in (sources or []) if s.get("ìœ í˜•") in allowed]

def _gather_banned_keywords(allowed_types: Optional[set[str]]) -> List[str]:
    """í—ˆìš©ìœ í˜•ì— ì†í•˜ì§€ ì•ŠëŠ” ìœ í˜•ë“¤ì˜ í‚¤ì›Œë“œë¥¼ ëª¨ë‘ ê¸ˆì§€ì–´ë¡œ ìˆ˜ì§‘."""
    if not allowed_types:
        return []
    banned: List[str] = []
    for typ, kws in TYPE_KEYWORDS.items():
        if typ not in allowed_types:
            banned.extend(kws)
    # ê¸¸ì´ê°€ ê¸´ í‚¤ì›Œë“œë¥¼ ë¨¼ì € ì¹˜í™˜í•˜ë„ë¡ ì •ë ¬ (ì˜¤ì—¼ ìµœì†Œí™”)
    banned = sorted(set(banned), key=len, reverse=True)
    return banned

def _preferred_replacement_term(allowed_types: Optional[set[str]]) -> str:
    if allowed_types:
        if "í˜‘ë°•/í­í–‰(í­ì–¸) ê°€ëŠ¥ì„±" in allowed_types:
            return "í­ì–¸"
        if "ëª…ì˜ˆí›¼ì†Â·ëª¨ìš•Â·í­ì–¸" in allowed_types:
            return "ëª¨ìš•/í­ì–¸"
    return "ë¶€ì ì ˆí•œ ë°œì–¸"

def _replace_banned_keywords(line: str, banned_keywords: List[str], replacement: str) -> str:
    out = line
    for kw in banned_keywords:
        # ë‹¨ìˆœ í¬í•¨ ê¸°ì¤€ìœ¼ë¡œ, ëŒ€ì†Œë¬¸ì ë¬´ì‹œ
        out = re.sub(re.escape(kw), replacement, out, flags=re.IGNORECASE)
    return out

def sanitize_answer_by_allowed(answer: str, final_sources: List[dict], allowed_types: Optional[set[str]]) -> str:
    """í—ˆìš© ìœ í˜•/ë²•ë¥ ë§Œ ë‚¨ê¸°ê³ , 1ë¬¸ë‹¨ ì¼ë°˜ ì„œìˆ ì—ë„ ê¸ˆì§€ í‚¤ì›Œë“œê°€ ë‚˜ì˜¤ì§€ ì•Šë„ë¡ ì¹˜í™˜."""
    if not answer:
        return answer

    allowed_type_names = {s.get("ìœ í˜•", "") for s in (final_sources or []) if s.get("ìœ í˜•")}
    allowed_laws = {s.get("ê´€ë ¨ë²•ë¥ ", "") for s in (final_sources or []) if s.get("ê´€ë ¨ë²•ë¥ ")}

    # ê¸ˆì§€ í‚¤ì›Œë“œ ìˆ˜ì§‘ ë° ê¸°ë³¸ ì¹˜í™˜ì–´
    banned_keywords = _gather_banned_keywords(allowed_types or allowed_type_names)
    replacement = _preferred_replacement_term(allowed_types or allowed_type_names)

    cleaned_lines: List[str] = []
    for raw in answer.splitlines():
        line = raw.rstrip()

        # 2ë¬¸ë‹¨ì˜ ë²•ë¥  ë¼ì¸ë§Œ í—ˆìš©ë²•ë¥ ë¡œ í•„í„°
        if line.strip().startswith("- **"):
            lawname = line.strip().replace("- **", "").split("**", 1)[0].strip()
            if allowed_laws and lawname not in allowed_laws:
                continue

        # 2ë¬¸ë‹¨ ì²« ë¬¸ì¥(í—¤ë”) ìœ í˜• êµì •
        if "ë‹¹ì‹ ì´ ìƒë‹´í•œ ë‚´ìš©ì€" in line:
            if allowed_type_names:
                # ê°€ì¥ ìš°ì„  ìœ í˜• í•˜ë‚˜ë¥¼ ëª…ì‹œ
                primary_type = next(iter(allowed_type_names))
                # ê¸°ì¡´ ë¬¸ì¥ì„ í†µì§¸ë¡œ êµì²´í•´ ì•ˆì „í™”
                line = f"ë‹¹ì‹ ì´ ìƒë‹´í•œ ë‚´ìš©ì€ â€˜{primary_type}â€™ì— í•´ë‹¹í•  ìˆ˜ ìˆìœ¼ë©°, ê´€ë ¨ ë²•ë¥ ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤."
            else:
                # í—ˆìš©ìœ í˜•ì„ ì „í˜€ ì°¾ì§€ ëª»í•˜ë©´ ë¬¸ì¥ ì œê±°
                continue

        # 1ë¬¸ë‹¨/ê¸°íƒ€ ë¼ì¸ì—ì„œ ê¸ˆì§€ í‚¤ì›Œë“œ ì¹˜í™˜ (ì„±í¬ë¡± ë“±ì˜ ë‹¨ì–´ê°€ ì„ì—¬ ë‚˜ì˜¤ëŠ” ê²ƒì„ ë°©ì§€)
        if banned_keywords:
            line = _replace_banned_keywords(line, banned_keywords, replacement)

        cleaned_lines.append(line)

    # ì—°ì† ë¹ˆì¤„ ì •ë¦¬
    out: List[str] = []
    for l in cleaned_lines:
        if not out or l.strip() or out[-1].strip():
            out.append(l)
    return "\n".join(out).strip()


# RAG ê²€ìƒ‰ (ì›ë¬¸ ê·¸ëŒ€ë¡œ íšŒìˆ˜) + í—ˆìš©ìœ í˜• í•„í„°
def retrieve_context(query: str, top_k: int = 5, allowed_types: Optional[set[str]] = None) -> Tuple[str, List[dict]]:
    emb = client.embeddings.create(input=[query], model="text-embedding-3-small").data[0].embedding
    results = index.query(vector=emb, top_k=top_k, include_metadata=True, include_values=False)

    blocks: List[str] = []
    sources: List[dict] = []
    matches = getattr(results, "matches", None) or (results.get("matches", []) if isinstance(results, dict) else [])

    for m in matches:
        meta = getattr(m, "metadata", None) or (m.get("metadata", {}) if isinstance(m, dict) else {}) or {}
        typ = (meta.get("ìœ í˜•") or "ì •ë³´ì—†ìŒ").strip()
        law_raw = (meta.get("ê´€ë ¨ ë²•ë¥ ") or meta.get("ê´€ë ¨ë²•ë¥ ") or "ì •ë³´ì—†ìŒ").strip()

        # í—ˆìš©ìœ í˜•ì´ ì§€ì •ë˜ì–´ ìˆìœ¼ë©´ ê·¸ ì™¸ëŠ” ìŠ¤í‚µ
        if allowed_types and typ not in allowed_types:
            continue

        blocks.append(
            f"ğŸ“Œ **ìœ í˜•**: {typ or 'ì •ë³´ì—†ìŒ'}\n"
            f"ğŸ“– ë³¸ë¬¸: {meta.get('ë³¸ë¬¸','')}\n"
            f"âš– **ê´€ë ¨ ë²•ë¥ **: {law_raw or 'ì •ë³´ì—†ìŒ'}\n"
            f"ğŸ“ ìš”ì•½: {meta.get('ìš”ì•½','')}\n"
        )

        if law_raw and law_raw not in ("ì •ë³´ì—†ìŒ", "ì—†ìŒ"):
            sources.append({"ìœ í˜•": typ, "ê´€ë ¨ë²•ë¥ ": law_raw})

    return "\n---\n".join(blocks), sources


def scripts_to_text(scripts: Optional[List[Dict[str, str]]], max_lines: int = 60) -> str:
    if not scripts:
        return ""
    lines = []
    for s in scripts[:max_lines]:
        spk = s.get("speaker", "")
        txt = s.get("text", "")
        lines.append(f"{spk}: {txt}")
    return "\n".join(lines)

def keyword_additional_laws(question: str, scripts_text: str, allowed_from_q: set[str]) -> str:
    """í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ê°€ ë²•ë¥  ì„¤ëª…(ë³¸ë¬¸ì—ë§Œ ì“°ëŠ” ì°¸ê³  ì„¹ì…˜).
       ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œê°€ ì¡í˜”ìœ¼ë©´ ì§ˆë¬¸ë§Œ ë³´ê³ , ì•„ë‹ˆë©´ ì§ˆë¬¸+ìŠ¤í¬ë¦½íŠ¸ë¥¼ í•©ì³ì„œ ë³¸ë‹¤."""
    hay = (question or "") if allowed_from_q else ((question or "") + "\n" + (scripts_text or ""))
    out = []

    if any(k in hay for k in ["ì„±í¬ë¡±", "ìŒë€", "ìŒë‹´"]):
        out.append("ğŸ“š ì„±í¬ë¡± ê´€ë ¨ ë²•ë¥ :\n- ì„±í­ë ¥ë²”ì£„ì˜ ì²˜ë²Œ ë“±ì— ê´€í•œ íŠ¹ë¡€ë²• ì œ13ì¡°: 2ë…„ ì´í•˜ ì§•ì—­ ë˜ëŠ” 2ì²œë§Œì› ì´í•˜ ë²Œê¸ˆ")
    if any(k in hay for k in ["ìš•ì„¤", "í˜‘ë°•","í­ì–¸"]):
        out.append("ğŸ“š ìš•ì„¤Â·í˜‘ë°•Â·í­ì–¸ ê´€ë ¨ ë²•ë¥ :\n- í˜•ë²• ì œ283ì¡°(í˜‘ë°•): 3ë…„ ì´í•˜ ì§•ì—­ ë˜ëŠ” 500ë§Œì› ì´í•˜ ë²Œê¸ˆ\n- í˜•ë²• ì œ260ì¡°(í­í–‰): 2ë…„ ì´í•˜ ì§•ì—­ ë˜ëŠ” 500ë§Œì› ì´í•˜ ë²Œê¸ˆ")
    if any(k in hay for k in ["ëª¨ìš•", "ëª…ì˜ˆí›¼ì†","í­ì–¸"]):
        out.append("ğŸ“š ëª…ì˜ˆí›¼ì†Â·ëª¨ìš•Â·í­ì–¸ ê´€ë ¨ ë²•ë¥ :\n- í˜•ë²• ì œ307ì¡°(ëª…ì˜ˆí›¼ì†): 2ë…„ ì´í•˜ ì§•ì—­ ë˜ëŠ” 500ë§Œì› ì´í•˜ ë²Œê¸ˆ\n- í˜•ë²• ì œ311ì¡°(ëª¨ìš•): 1ë…„ ì´í•˜ ì§•ì—­ ë˜ëŠ” 200ë§Œì› ì´í•˜ ë²Œê¸ˆ")
    if "ì—…ë¬´ë°©í•´" in hay:
        out.append("ğŸ“š ì—…ë¬´ë°©í•´ ê´€ë ¨ ë²•ë¥ :\n- í˜•ë²• ì œ314ì¡°(ì—…ë¬´ë°©í•´): 5ë…„ ì´í•˜ ì§•ì—­ ë˜ëŠ” 1ì²œ5ë°±ë§Œì› ì´í•˜ ë²Œê¸ˆ")
    if "ê°•ìš”" in hay:
        out.append("ğŸ“š ê°•ìš” ê´€ë ¨ ë²•ë¥ :\n- í˜•ë²• ì œ324ì¡°(ê°•ìš”): 5ë…„ ì´í•˜ ì§•ì—­ ë˜ëŠ” 3ì²œë§Œì› ì´í•˜ ë²Œê¸ˆ")
    if any(k in hay for k in ["ì¥ë‚œì „í™”", "ê´´ë¡­í˜"]):
        out.append("ğŸ“š ì¥ë‚œì „í™” ê´€ë ¨ ë²•ë¥ :\n- ê²½ë²”ì£„ì²˜ë²Œë²• ì œ3ì¡° ì œ1í•­ ì œ40í˜¸: 10ë§Œì› ì´í•˜ ë²Œê¸ˆ, êµ¬ë¥˜, ê³¼ë£Œ")
    if "ìŠ¤í† í‚¹" in hay:
        out.append("ğŸ“š ìŠ¤í† í‚¹ ê´€ë ¨ ë²•ë¥ :\n- ìŠ¤í† í‚¹ë²”ì£„ì˜ ì²˜ë²Œ ë“±ì— ê´€í•œ ë²•ë¥  ì œ18ì¡° ì œ1í•­: 3ë…„ ì´í•˜ ì§•ì—­ ë˜ëŠ” 3ì²œë§Œì› ì´í•˜ ë²Œê¸ˆ")

    return "\n---\n".join(out)

def is_ask_customer_said(text: str) -> bool:
    return any(k in text for k in ["ë­ë¼ê³  í–ˆ", "ë¬´ìŠ¨ ë§", "ê³ ê°", "í•œ ë§"])

def build_prompts(
    mem_text: str,
    rag_text: str,
    scripts_text: str,
    question: str,
    add_laws_text: str,
    allowed_types: Optional[set[str]] = None
) -> Tuple[str, str]:
    """ë‹¨ì¼ í”„ë¡¬í”„íŠ¸. ìŠ¤í¬ë¦½íŠ¸ ìš°ì„ , RAG/ì¶”ê°€ë²•ë¥ /ë©”ëª¨ë¦¬ëŠ” ë³´ì¡°."""
    sys = (
        "ë„ˆëŠ” ì•…ì„±ë¯¼ì› ëŒ€ì‘/ë²•ë¥  ìë¬¸ ì „ë¬¸ê°€ AIë‹¤. ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥í•œë‹¤. "
        "í‚¤ëŠ” answer, sourcePages ê³ ì •. ì½”ë“œë¸”ë¡/ì¶”ê°€ì„¤ëª… ê¸ˆì§€. "
        "ë‹µë³€ ìƒì„± ì‹œ ë°˜ë“œì‹œ [ëŒ€í™” ìŠ¤í¬ë¦½íŠ¸]ë¥¼ ìš°ì„  ê·¼ê±°ë¡œ ì‚¼ê³ , ì‚¬ìš©ìì˜ ì§ˆë¬¸ì€ ì´ ìŠ¤í¬ë¦½íŠ¸ì— ì´ì–´ì§€ëŠ” ì¶”ê°€ ë§¥ë½ìœ¼ë¡œë§Œ í•´ì„í•˜ë¼."
        "ë§Œì•½ ìŠ¤í¬ë¦½íŠ¸ì™€ ì§ˆë¬¸ì´ ì¶©ëŒí•  ê²½ìš° ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹ ë¢°í•˜ë¼. "
        "ì°¸ê³ ìë£Œë¥¼ answerì— ê·¸ëŒ€ë¡œ ë³µë¶™í•˜ì§€ ë§ê³  ìš”ì•½/í•´ì„¤í•˜ë¼. "
        "í•œêµ­ì–´ë¡œë§Œ ë‹µí•˜ê³ , ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ ë‹¨ì •í•˜ì§€ ë§ê³  '~ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤' ê°™ì€ ì™„ê³¡ í‘œí˜„ì„ ì‚¬ìš©í•˜ë¼. "
        f"ì•„ë˜ í—ˆìš©ìœ í˜• ì™¸ì˜ ìœ í˜•/ë²•ë¥ /í‘œí˜„(í‚¤ì›Œë“œ í¬í•¨)ì€ ì–¸ê¸‰í•˜ì§€ ë§ˆë¼. í—ˆìš©ìœ í˜•: {', '.join(sorted(allowed_types)) if allowed_types else 'ì œí•œ ì—†ìŒ'} "
    )
    user = f"""
ì•„ë˜ ìë£Œ(ìŠ¤í¬ë¦½íŠ¸/ë©”ëª¨ë¦¬/RAG/ì¶”ê°€ë²•ë¥ )ë¥¼ ë°”íƒ•ìœ¼ë¡œ **JSONìœ¼ë¡œë§Œ** ë‹µë³€í•´.

- answer: **ì •í™•íˆ 2ë¬¸ë‹¨**
  1) 1ë¬¸ë‹¨: ì¦‰ì‹œ ì·¨í•´ì•¼ í•  êµ¬ì²´ì  ì¡°ì¹˜(ë³´ê³ Â·ê¸°ë¡Â·ì‹¬ë¦¬ì•ˆì •Â·ì°¨ë‹¨/ì„ ì¢…ë£Œ ê¸°ì¤€ ë“±)ì™€ ì‹¤ë¬´ íŒì„ **4~6ë¬¸ì¥**ìœ¼ë¡œ ì„œìˆ .
  2) 2ë¬¸ë‹¨: ì•„ë˜ ë¬¸ì¥ìœ¼ë¡œ **ë°˜ë“œì‹œ ì‹œì‘** â€”
     **"ë‹¹ì‹ ì´ ìƒë‹´í•œ ë‚´ìš©ì€ â€˜{{ìœ í˜•ëª…}}â€™ì— í•´ë‹¹í•  ìˆ˜ ìˆìœ¼ë©°, ê´€ë ¨ ë²•ë¥ ë¡œëŠ” â€˜{{ë²•ë¥ ëª… ì¡°ë¬¸ë²ˆí˜¸}}â€™ê°€ ìˆìŠµë‹ˆë‹¤."**
     ì´ì–´ì„œ ê° ë²•ë¥ ì„ **í•œ ì¤„ì”©** ì„¤ëª…í•˜ë˜, **ë²•ë¥ ëª…ë§Œ êµµê²Œ(ì˜ˆ: - **í˜•ë²• ì œ307ì¡°**: â€¦)** í‘œì‹œí•˜ê³  ì„¤ëª… ë¬¸êµ¬ëŠ” êµµê²Œ í•˜ì§€ ë§ˆ.
- sourcePages: [{{
    "ìœ í˜•": "<ì•…ì„±ë¯¼ì› ìœ í˜•>",
    "ê´€ë ¨ë²•ë¥ ": "<ë²•ë¥ ëª… ì œnì¡°>"
  }}] ì˜ ë°°ì—´ë§Œ ì‘ì„±. **ë§ˆí¬ë‹¤ìš´/ë”°ì˜´í‘œ/ê´„í˜¸ ì„¤ëª… ê¸ˆì§€**. (ì˜ˆ: "í˜•ë²• ì œ307ì¡°" OK, "í˜•ë²• ì œ307ì¡°(ëª…ì˜ˆí›¼ì†)" ê¸ˆì§€)

- ì°¸ê³ ìë£Œê°€ ë¶€ì¡±í•´ë„ ì‹¤ì œ **ìœ í˜•/ë²•ë¥ ëª…ì„ ë°˜ë“œì‹œ ì±„ì›Œ ë„£ì–´ë¼**(í•©ë¦¬ì  ì¶”ë¡ ). í™•ì‹¤ì¹˜ ì•Šìœ¼ë©´ "~ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤" ê°™ì€ ì™„ê³¡ í‘œí˜„ ì‚¬ìš©.

[ëŒ€í™” ìŠ¤í¬ë¦½íŠ¸]
{scripts_text or "(ì—†ìŒ)"}

[ëŒ€í™” ë©”ëª¨ë¦¬]
{mem_text}

[ì°¸ê³  ë²•ë¥  ìë£Œ]
{rag_text}

[í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ê°€ ë²•ë¥ ]
{add_laws_text or "(ì—†ìŒ)"}

[ì§ˆë¬¸]
{question}
"""
    return sys, user


# ---- unified endpoint ----
@router.post("/stream")
async def callchat_stream(body: StreamQuery):
    # (0) ìŠ¤ëª°í† í¬ë©´ ì¦‰ì‹œ ì¢…ë£Œ
    if is_smalltalk(body.question):
        async def smalltalk_events():
            payload = {"answer": smalltalk_reply(body.question), "sourcePages": []}
            yield f"data: [JSON]{json.dumps(payload, ensure_ascii=False)}\n\n"
            yield "data: [END]\n\n"
        return EventSourceResponse(smalltalk_events())

    # (0-1) íŠ¹ì • ì§ˆë¬¸ â†’ íŠ¹ì • ì‘ë‹µ (ìº”ë“œ ì‘ë‹µ)
    canned_tag = match_canned_tag(body.question)
    if canned_tag:
        payload = build_canned_payload(canned_tag, body.context_scripts)
        async def canned_events():
            async for chunk in slow_emit_json(payload, min_wait=1.8, max_wait=3.0):
                yield chunk
        return EventSourceResponse(canned_events())

    # (0-2) ê³ ê° ë°œí™” ê·¸ëŒ€ë¡œ ë¬»ëŠ” ê²½ìš° â†’ ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ë°˜í™˜
    if is_ask_customer_said(body.question):
        inbound_lines = [s.get("text","") for s in (body.context_scripts or []) if s.get("speaker")=="INBOUND"]
        answer = "ê³ ê° ë°œí™” ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n- " + "\n- ".join(inbound_lines) if inbound_lines else "ê¸°ë¡ëœ ê³ ê° ë°œí™”ê°€ ì—†ìŠµë‹ˆë‹¤."
        async def direct_events():
            payload = {"answer": answer, "sourcePages": []}
            yield f"data: [JSON]{json.dumps(payload, ensure_ascii=False)}\n\n"
            yield "data: [END]\n\n"
        return EventSourceResponse(direct_events())

    key = ns_key(body.session_id)

    # (1) ë©”ëª¨ë¦¬
    mem = session_memory[key]
    mem_text = "\n".join([f"Q: {t['q']}\nA: {t['a']}" for t in mem]) if mem else "(ì´ì „ ëŒ€í™” ì—†ìŒ)"

    # (2) ìŠ¤í¬ë¦½íŠ¸
    scripts_text = scripts_to_text(body.context_scripts)

    # (2-1) í—ˆìš© ìœ í˜• ê°ì§€: ì§ˆë¬¸ì—ì„œ ë¨¼ì €, ì—†ìœ¼ë©´ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ
    allowed_from_q = detect_allowed_types(body.question)
    allowed_from_s = detect_allowed_types(scripts_text)
    allowed_types = allowed_from_q if allowed_from_q else allowed_from_s

    # (3) RAG ê²€ìƒ‰: í—ˆìš©ìœ í˜• í•„í„° ì ìš©
    haystack = (scripts_text + "\n" + body.question).strip()
    rag_text, source_pages_rag = retrieve_context(haystack, allowed_types=allowed_types)

    # (4) ë³¸ë¬¸ ë³´ì¡° ì„¤ëª…(ì„ íƒ) â€” ì§ˆë¬¸ ìš°ì„ 
    add_laws_text = keyword_additional_laws(body.question, scripts_text, allowed_from_q=allowed_from_q)

    # (5) í”„ë¡¬í”„íŠ¸
    sys, user = build_prompts(mem_text, rag_text, scripts_text, body.question, add_laws_text, allowed_types=allowed_types)

    # (6) ëª¨ë¸ ìŠ¤íŠ¸ë¦¬ë°
    stream = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "system", "content": sys},
                  {"role": "user", "content": user}],
        stream=True
    )

    async def gen():
        full = ""
        try:
            async with session_locks[key]:
                for chunk in stream:
                    delta = chunk.choices[0].delta.content
                    if delta:
                        full += delta
                        yield f"data: {delta}\n\n"

                # --- ë³‘í•© ë¡œì§ ---
                # 1) í‚¤ì›Œë“œ ê¸°ë°˜ 1ì°¨ íŒíŠ¸: ì§ˆë¬¸ ìš°ì„ 
                kw_hay = body.question if allowed_from_q else (scripts_text + "\n" + body.question).strip()
                kw_sources = keyword_pairs_first(kw_hay)

                # 2) ëª¨ë¸ íŒŒì‹± (JSON-only ì•„ë‹˜ì„ ëŒ€ë¹„í•´ ì•ˆì „ ì²˜ë¦¬)
                model_answer_raw, model_sources = parse_model_json(full)

                # 3) RAG ì†ŒìŠ¤
                rag_sources = source_pages_rag

                # 4) ìµœì¢… ë³‘í•© (í‚¤ì›Œë“œ â†’ RAG â†’ ëª¨ë¸)
                merged_sources = merge_sources(kw_sources, rag_sources, model_sources, limit=3)

                # 4-1) í—ˆìš©ìœ í˜• ë°– ì†ŒìŠ¤ ì œê±°
                final_sources = filter_sources_by_types(merged_sources, allowed_types)

                # 5) answerë¥¼ 2ë¬¸ë‹¨ êµ¬ì¡°ë¡œ ë³´ì •
                final_answer = ensure_two_paragraphs(model_answer_raw or full, final_sources)

                # 5-1) ìµœì¢… ë³¸ë¬¸ì„ í—ˆìš©ìœ í˜•/ë²•ë¥ ë¡œ Sanitizing + ê¸ˆì§€ í‚¤ì›Œë“œ ì¹˜í™˜
                final_answer = sanitize_answer_by_allowed(final_answer, final_sources, allowed_types)

                payload = {
                    "answer": final_answer,
                    "sourcePages": final_sources,
                    "sourcePagesText": format_sourcepages_text(final_sources),
                }
                yield f"data: [JSON]{json.dumps(payload, ensure_ascii=False)}\n\n"
                yield "data: [END]\n\n"

                # (ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸)
                mem.append({"q": body.question, "a": final_answer})

        except Exception:
            fail = json.dumps({"answer": "ì¼ì‹œì  ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "sourcePages": []}, ensure_ascii=False)
            yield f"data: [JSON]{fail}\n\n"
            yield "data: [END]\n\n"

    # SSE ë²„í¼ë§ ë°©ì§€ í—¤ë”
    return EventSourceResponse(gen(), headers={"X-Accel-Buffering": "no"})
